---
title: "Estimation"
output:
  html_notebook:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(emphasis) # Load the Emphasis package


# Install and load the required packages
if (!requireNamespace("ape", quietly = TRUE)) {
  install.packages("ape")
}
library(ape)

```

On this noteboook, we will explore different estimation methods on species diversification processes. We will focuss on estimating on the PD model

The Phylogenetic Diversification (PD) model describes the speciation rate over time within a phylogenetic tree. The model takes into account both speciation and extinction events, and it can be mathematically described by the following equations:

The speciation rate $\lambda_s(t)$ at time $t$ is given by:

$$
\lambda_s(t | \lambda_0, \beta_N, \beta_P) = \max \left( 0, \lambda_0 + \beta_N N_t + \beta_P \frac{P_t}{N_t} \right);
$$

where:

-   $\lambda_0$ is the baseline speciation rate,
-   $\beta_N$ is the coefficient for the effect of the number of lineages at time $t$ (denoted $N_t$),
-   $\beta_P$ is the coefficient for the effect of the phylogenetic diversity at time $t$ (denoted $P_t$), normalized by the number of lineages $N_t$.

The extinction rate $\mu_s(t)$ is assumed to be constant over time, denoted by $\mu_0$:

$$
\mu_s(t | \mu_0) = \mu_0
$$

The PD model ensures that the speciation rate is non-negative and allows for the simulation of complex evolutionary scenarios.

```{r}

library(rotl)
taxon_name = "Primates"

matched_names <- tnrs_match_names(taxon_name)
taxon_id <- matched_names$ott_id[1]
  
# Retrieve the subtree for the taxon
phylo <- tol_subtree(ott_id = taxon_id)

plot(phylo)
```

Phylogenies that are derived from molecular data (e.g. DNA sequences) are, however, not full trees, as they do not contain the extinct species. The likelihood for an observed tree can be written in terms of the likelihood of compatible full trees. In principle, this is simply the integration over all possible full trees that are compatible with the observed tree $x_{obs}$:

```{=tex}
\begin{equation}
    f(x_{obs}|\theta)  =  \displaystyle\int_{x \in \mathcal{X}(x_{obs})}   \exp(\ell _{x} (\theta|x_{obs})) dx 
    \label{logy}
\end{equation}
```
This integration is usually impossible to compute in practice for most diversification models (\ref{logy}). We can obtain a numerical approximation by using the Monte-Carlo approach considering

\begin{eqnarray} 
    f (x_{obs}|\theta) &=& \displaystyle\int_{x \in \mathcal{X}(x_{obs})} f (x|\theta) dx \nonumber \\ 
    &=&\displaystyle\int_{x \in \mathcal{X}(x_{obs})} \frac{f (x|\theta)}{f_m (x|\theta,x_{obs})} f_m (x|\theta,x_{obs}) dx \nonumber \\ 
    &\approx& \frac{1}{M}\sum_{x_i \sim f_m (x|\theta,x_{obs}) } \frac{f (x_i|\theta)}{f_m (x_i|\theta,x_{obs})} 
    \label{MC}
\end{eqnarray} for $f (x|\theta) = \exp(\ell _{x} (\theta|x_{obs}))$, $M$ the Monte-Carlo sample size and $f_m$ is an arbitrary sampler of the missing part of the full tree given an extant tree $x_{obs}$. Our statistical framework is a generalisation of that of, which makes use of an Expectation-Maximization algorithm for maximising the likelihood

### Likelihood function

```{r}
brts <- ape::branching.times(phylo)

# Define parameter bounds
lower_bound = c(0,0.5,-0.1,0)
upper_bound = c(1,3,0,0.1)

num_points <- 1000

pars <- matrix(nrow = num_points, ncol = length(lower_bound))
for (i in seq_along(lower_bound)) {
  pars[, i] <- runif(num_points, min = lower_bound[i], max = upper_bound[i])
}

dmval <- mcGrid(pars, 
                brts = brts, 
                sample_size = 1, 
                maxN = 1, 
                soc = 2, 
                max_missing = 1e+4, 
                max_lambda = 1e+4, 
                lower_bound = lower_bound, 
                upper_bound = upper_bound, 
                xtol_rel = 0.01, 
                num_threads = 4)
# Extract relevant columns
loglikelihood_est <- dmval[, 1]
sampled_x_loglikelihood <- dmval[, 5]
sampling_probability <- dmval[, 6]

# Remove rows with NA in loglikelihood_est
valid_indices <- !is.na(loglikelihood_est)
loglikelihood_est <- loglikelihood_est[valid_indices]
sampled_x_loglikelihood <- sampled_x_loglikelihood[valid_indices]
sampling_probability <- sampling_probability[valid_indices]

# Plotting the distribution of loglikelihood estimates
hist(sampling_probability/loglikelihood_est, main="Distribution of Loglikelihood Estimates", xlab="Loglikelihood Estimate")

# Scatter plot of sampled_x_loglikelihood vs sampling_probability
plot(sampled_x_loglikelihood, sampling_probability, main="Sampled Loglikelihood vs Sampling Probability",
     xlab="Sampled Loglikelihood", ylab="Sampling Probability", pch=19)

# Adding a line to visualize the ideal sampling distribution might require additional theoretical information
# For simplicity, this example focuses on the distributions and relationships in the sampled data


# Assuming dmval is your matrix
plot(dmval[,5], dmval[,6], main = "Loglikelihood vs. Sampling Probability",
     xlab = "Loglikelihood of Sampled x", ylab = "Sampling Probability",
     pch = 19, col = "blue")

# Adding a 1:1 line for perfect agreement
abline(a = 0, b = 1, col = "red")

# Calculate correlation
correlation <- cor(dmval[,5], dmval[,6], use = "complete.obs")
cat("Correlation between Loglikelihood of Sampled x and Sampling Probability: ", correlation, "\n")
```

## GAM

One option to estimate the full function is to use Generalized Additive Models. We train a GAM using the `train_GAM` function

```{r}
gam1 = train_GAM(results)
```

And we visualize how good it fits with summary results

```{r}
print(gam1)
summary(gam1)

```

We can see that the GAM is trained and all parameters significant. Let's visuzalize the estimated likelihood

```{r}
vis.gam(gam1)  
vis.gam(gam1,view=c("betaN","betaP"))
```

Now let's do some test. The PD model is a generalization of the DD model, for which we do have the calculation of an analitical solution.

```{r}
DDD:::dd_ML(brts = branching.times(phylo))
```

```{r}

data_point = data.frame(mu = 0.104095, lambda = 0.813530, betaN = (0.104095- 0.813530)/112.471829, betaP=0)
predict(gam1, newdata = data_point, type = "response")

```

Prediction is not that close, let's see more points

```{r}
# Generate a grid of points within these ranges
npoints = 300
grid_data <- data.frame(
  mu = runif(npoints, min = mu_interval[1], max = mu_interval[2]),
  lambda = runif(npoints, min = lambda_interval[1], max = lambda_interval[2]),
  betaN = rep(120,npoints),
  betaP = rep(0,npoints)
)

grid_data$betaN  =  - abs(grid_data$mu-grid_data$lambda)/grid_data$betaN

# Predict log likelihood for each grid
grid_data$loglik_pred <- as.numeric(predict(gam1, newdata = grid_data, type = "response"))


loglik = NULL
pars2 = c(1000,1,0,1,0,2)
for(i in 1:nrow(grid_data)){
  pars1 = abs(c(grid_data$lambda[i], grid_data$mu[i], (grid_data$lambda[i] - grid_data$mu[i])/grid_data$betaN[i]))
  ll = DDD:::dd_loglik(pars1 = pars1,
                                     pars2 = pars2,
                                     brts = as.numeric(branching.times(phylo)),
                                     missnumspec = 0)
  loglik = c(loglik, ll)

}
```


```{r}
plot(loglik,grid_data$loglik_pred)
abline(a = 0,b = 1)
```

Now, we can use the predictive function to find an estimation of the MLE.

```{r}
dd_ML_est(gam1)
```

## DE

### Conditioning

## MCEM

```{r}
pars = c(0.25,  0.80, -0.01,  0)
max_t = 20

max_lin = 100000
max_tries = 1

#sim_single_tree_pd_cpp
tree = sim_tree_pd_cpp(pars = pars,
                       max_t = 15,
                       max_lin = max_lin,
                       max_tries = max_tries)

plot(tree$tes)

brts <- tree$brts


# Define lower and upper bounds for the parameters
lower_bound <- c(0, 0.5, -0.05, -0.05)
upper_bound <- c(0.5, 2, 0.01, 0.03)

# Other settings
max_lambda <- 1000
xtol <- 0.001
em_tol <- 0.25
sample_size_tol <- 0.005
verbose <- TRUE
return_trees <- FALSE
max_missing <- 10000

# Number of iterations and points for DE algorithm
num_iterations <- 10
num_points <- 100
disc_prop <- 0.75
maxN <- 100

# Call the emphasis function
result <- emphasis(brts = brts,init_par = maxN model, lower_bound, upper_bound, max_lambda, xtol, 
                   em_tol, sample_size_tol, verbose, return_trees, max_missing, 
                   num_iterations, num_points, disc_prop, maxN)

# Examine the results
print(result$pars)



```



## GMM
